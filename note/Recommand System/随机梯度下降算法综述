随机梯度下降法综述 作者：Sebastian Ruder
论文中文版：https://wenku.baidu.com/view/d1c0d86630b765ce0508763231126edb6f1a76a1.html

简介：梯度下降法  
    批梯度下降，随机梯度下降，小批量梯度下降；
   样本量大时，梯度计算非常耗时，每次都使用完全相同的样本集导致冗余计算；
   学习率不好调整；
随机梯度下降
    避免了冗余，但震荡比较大，反而更容易得到全局最优；
	在逐渐缩小学习率的情况下，与批梯度下降收敛速度类似；
随机梯度下降的问题与挑战
    主要困难在于学习率的选取；
随机梯度下降的优化算法（主要内容）
    动量法：引入一个较大的质量，震荡次数减小；
	   改进动量法：预判下一个位置后再算梯度，避免在主方向上震荡；
	Adagrad自动调整学习率：更新频繁的参数使用较小的学习率；
	   Adamdelta改进Adagrad：避免学习率衰减过快，使用移动平均取代全部历史平方和，时间越近的值权值大；
	Adam结合了动量法和Adamdelta算法：
	   一阶情况下的动量，二阶情况下的震荡频率；
并行与分布式架构
随机梯度下降的其他优化方法
    洗牌与数据排序
	   每次循环前洗牌，避免数据顺序影响参数更新；
	批规范化
	   确保每一层的输入数据都是规范化后的数据；
	Early Stopping
	   当误差减小程度小于阈值时，终止训练；
	Gradient Noise
	    在每次更新中加入噪声项，跳出鞍点和局部最小值点；
